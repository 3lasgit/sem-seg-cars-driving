{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmantation for cars driving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tch\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/blidi/OneDrive/Bureau/projet/semantic_segmentation/data_semantics/training/image_2')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = Path(\"%pwd\").resolve().parent\n",
    "path_data = dir / \"data_semantics/training/image_2/\"\n",
    "path_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determining the cropping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images do not have the same shape so we will crop it to the minimum for all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 370, 1224,    3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vecteur pour stocker les tailles minimales des images\n",
    "min_image_sizes = []\n",
    "\n",
    "# Parcours de chaque fichier dans le répertoire\n",
    "for filename in os.listdir(path_data):\n",
    "    # Vérifiez si le fichier est une image\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Construisez le chemin complet du fichier\n",
    "        filepath = os.path.join(path_data, filename)\n",
    "        # Utilisez la déclaration with pour ouvrir le fichier\n",
    "        with open(filepath, 'rb') as f:    \n",
    "            # Chargez l'image avec OpenCV\n",
    "            img = cv2.imread(os.path.join(path_data, filename))\n",
    "            # Convertissez l'image en BGR en RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Ajoutez la taille de l'image au vecteur des tailles minimales\n",
    "            min_image_sizes.append(img.shape)\n",
    "\n",
    "# Convertissez le vecteur des tailles minimales en un tenseur PyTorch\n",
    "min_image_sizes = tch.tensor(min_image_sizes).min(0).values # don't need the indices\n",
    "min_image_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 375, 1242] at entry 0 and [3, 370, 1224] at entry 155",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[4], line 28\u001b[0m\n",
      "\u001b[0;32m     25\u001b[0m             images_tensor\u001b[38;5;241m.\u001b[39mappend(img_tensor)\n",
      "\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Convertissez la liste de tenseurs en un seul tenseur contenant toutes les images\u001b[39;00m\n",
      "\u001b[1;32m---> 28\u001b[0m images_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 375, 1242] at entry 0 and [3, 370, 1224] at entry 155"
     ]
    }
   ],
   "source": [
    "# Liste pour stocker les images en tant que tenseurs\n",
    "images_tensor = []\n",
    "\n",
    "# Parcours de chaque fichier dans le répertoire\n",
    "for filename in os.listdir(path_data):\n",
    "    # Vérifiez si le fichier est une image\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Construisez le chemin complet du fichier\n",
    "        filepath = os.path.join(path_data, filename)\n",
    "        # Utilisez la déclaration with pour ouvrir le fichier\n",
    "        with open(filepath, 'rb') as f:    \n",
    "            # Chargez l'image avec OpenCV\n",
    "            img = cv2.imread(os.path.join(path_data, filename))\n",
    "            # Convertissez l'image en BGR en RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Convertissez l'image en tenseur Pytorch\n",
    "            img_tensor = tch.from_numpy(img)  # Transposez les dimensions pour obtenir le format correct (C x H x W)\n",
    "            img_tensor = img_tensor.float() / 255.0  # Normalisez les valeurs des pixels entre 0 et 1\n",
    "            # Ajoutez la taille minimale de l'image au vecteur des tailles minimales\n",
    "            min_image_sizes.append(img.shape)\n",
    "            # Ajoutez le tenseur de l'image à la liste\n",
    "            images_tensor.append(img_tensor)\n",
    "\n",
    "# Convertissez le vecteur des tailles minimales en un tenseur PyTorch\n",
    "min_image_sizes_tensor = tch.tensor(min_image_sizes)\n",
    "min_image_sizes_tensor.min(0)\n",
    "\n",
    "# Convertissez la liste de tenseurs en un seul tenseur contenant toutes les images\n",
    "images_tensor = tch.stack(images_tensor, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masks images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/blidi/OneDrive/Bureau/projet/semantic_segmentation/data_semantics/training/semantic')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir = Path(\"%pwd\").resolve().parent\n",
    "path_data = dir / \"data_semantics/training/semantic\"\n",
    "path_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste pour stocker les images en tant que tenseurs\n",
    "images_tensor = []\n",
    "\n",
    "# Vecteur pour stocker les tailles minimales des images\n",
    "min_image_sizes = []\n",
    "\n",
    "# Parcours de chaque fichier dans le répertoire\n",
    "for filename in os.listdir(path_data):\n",
    "    # Vérifiez si le fichier est une image\n",
    "    if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "        # Construisez le chemin complet du fichier\n",
    "        filepath = os.path.join(path_data, filename)\n",
    "        # Utilisez la déclaration with pour ouvrir le fichier\n",
    "        with open(filepath, 'rb') as f:    \n",
    "            # Chargez l'image avec OpenCV\n",
    "            img = cv2.imread(os.path.join(path_data, filename))\n",
    "            # Convertissez l'image en BGR en RGB\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            # Convertissez l'image en tenseur Pytorch\n",
    "            img_tensor = tch.from_numpy(img.transpose(2, 0, 1))  # Transposez les dimensions pour obtenir le format correct (C x H x W)\n",
    "            img_tensor = img_tensor.float() / 255.0  # Normalisez les valeurs des pixels entre 0 et 1\n",
    "            # Ajoutez la taille minimale de l'image au vecteur des tailles minimales\n",
    "            min_image_sizes.append(img.shape)\n",
    "            # Ajoutez le tenseur de l'image à la liste\n",
    "            images_tensor.append(img_tensor)\n",
    "\n",
    "# Convertissez la liste de tenseurs en un seul tenseur contenant toutes les images\n",
    "taille =  []\n",
    "for i in images_tensor :\n",
    "    taille.append(i.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.min(\n",
       "values=tensor([ 370, 1224,    3]),\n",
       "indices=tensor([155, 155,   0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertissez le vecteur des tailles minimales en un tenseur PyTorch\n",
    "min_image_sizes_tensor = tch.tensor(min_image_sizes)\n",
    "min_image_sizes_tensor.min(0)\n",
    "#taille_uniforme = tch.tensor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
