{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da5893a-2e5e-48b2-9ef1-e573611d54fb",
   "metadata": {},
   "source": [
    "## Prerequesites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d03fd7cb-9083-40e5-858d-54aa2bb162fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as tch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e54299d6-b08c-4f4e-a1da-0e8cd49c89d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "project_id = \"semantic-segmentation-on-kitti\" # @param {type:\"string\"}\n",
    "# Set the project id\n",
    "! gcloud config set project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "538bb6b8-1fb4-48d0-90af-de284d6b8488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125b7be0-d446-4632-b990-2a9eace3d50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://semantic-segmentation-on-kitti-aip-20240328170341\n"
     ]
    }
   ],
   "source": [
    "bucket_name = \"bucket-name-placeholder\"  # @param {type:\"string\"}\n",
    "bucket_uri = f\"gs://{bucket_name}\"\n",
    "\n",
    "bucket_name = \"semantic-segmentation-on-kitti-aip-20240328170341\"\n",
    "bucket_uri = \"gs://semantic-segmentation-on-kitti-aip-20240328170341\"\n",
    "\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "if bucket_name == \"\" or bucket_name is None or bucket_name == \"bucket-name-placeholder\":\n",
    "    bucket_name = project_id + \"-aip-\" + timestamp\n",
    "    bucket_uri = \"gs://\" + bucket_name\n",
    "! echo $bucket_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1103f9f-371e-47aa-96a2-b6a78b5304ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bucket_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m client \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mClient(project\u001b[38;5;241m=\u001b[39mproject_id)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Create a bucket\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m bucket \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mget_bucket(\u001b[43mbucket_name\u001b[49m) \u001b[38;5;66;03m# client.create_bucket(bucket_name, location=region)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bucket_name' is not defined"
     ]
    }
   ],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client(project=project_id)\n",
    "\n",
    "# Create a bucket\n",
    "bucket = client.get_bucket(bucket_name) # client.create_bucket(bucket_name, location=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7473a1ad-3243-4a40-8e04-f38ee870909d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket semantic-segmentation-on-kitti-aip-20240328170341 created.\n"
     ]
    }
   ],
   "source": [
    "print(\"Bucket {} created.\".format(bucket.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89c3cf9-03ca-4ef4-8b10-b56bc8866737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=project_id, location=region, staging_bucket=bucket_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7785da-6ad7-45be-8e7c-6a700b839e5d",
   "metadata": {},
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3ead584-cc21-46f4-9676-273496b0c6fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task_unet_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task_unet_model.py\n",
    "\n",
    "\n",
    "import torch as tch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Importing the data \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr_rate', default=0.001, type=int)\n",
    "parser.add_argument('--n_epoch', default=100, type=int)\n",
    "parser.add_argument('--batch_size', default=1, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Add saving and loading models for big epochs' training\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# Initialiser le client GCS\n",
    "\n",
    "#project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "project_id = \"semantic-segmentation-on-kitti\"\n",
    "client = storage.Client(project=project_id)\n",
    "\n",
    "\n",
    "bucket_name = 'data_kitti_driv_seg'\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "# Liste des objets dans le bucket\n",
    "blobs = bucket.list_blobs()\n",
    "\n",
    "    \n",
    "from io import BytesIO\n",
    "\n",
    "# Récupérer l'objet depuis le bucket\n",
    "object_path = 'data/training_tensor.pt'\n",
    "blob = bucket.blob(object_path)\n",
    "# Télécharger les données de l'objet en mémoire\n",
    "data = BytesIO(blob.download_as_string())\n",
    "\n",
    "training_tensor = tch.load(data)\n",
    "training_tensor.shape\n",
    "\n",
    "\n",
    "# Constructing the dataset objects\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, data_tensor):\n",
    "        self.data = data_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]  # Nombre d'exemples dans le tensor data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Extraire l'image et le masque correspondant à l'index donné\n",
    "        image = self.data[0, index]  # Première dimension pour les images\n",
    "        mask = self.data[1, index]   # Deuxième dimension pour les masques\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "# Splitting data into training/test datasets\n",
    "\n",
    "training_data, test_data = ImageMaskDataset(training_tensor[:,:10]), ImageMaskDataset(training_tensor[:,10:])\n",
    "\n",
    "# Création du DataLoader\n",
    "data_loader = tch.utils.data.DataLoader(training_data, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Building the CNN (U-Net)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module): # Creating a class merging the double conv\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),          # X_out=X_in cf formula applied with these parameters' values\n",
    "            nn.BatchNorm2d(out_channels),                                                # keeps size\n",
    "            nn.ReLU(inplace=True),                                                       # keeps size \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),         \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                                                                # Keeps the same image size of the input\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.dconv_down1 = DoubleConv(in_channels, 64)        # keeps image size \n",
    "        self.dconv_down2 = DoubleConv(64, 128)                # keeps image size \n",
    "        self.dconv_down3 = DoubleConv(128, 256)               # keeps image size \n",
    "        self.dconv_down4 = DoubleConv(256, 512)               # keeps image size \n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2)          # X_out=int((X_in/2) + 1)   # Caution : default stride is equal to kernel-size here\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # X_out=int(X_in*2)       # Fasten the process as it hasn't to learn weights unlike the convtranspose (which is so )\n",
    "        \n",
    "        self.dconv_up3 = DoubleConv(256 + 512, 256)          # keeps image size \n",
    "        self.dconv_up2 = DoubleConv(128 + 256, 128)          # keeps image size\n",
    "        self.dconv_up1 = DoubleConv(128 + 64, 64)            # keeps image size\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)      # keeps image size\n",
    "\n",
    "    def forward(self, x): \n",
    "        conv1 = self.dconv_down1(x)          \n",
    "        x = self.maxpool(conv1)     \n",
    "\n",
    "        conv2 = self.dconv_down2(x)          \n",
    "        x = self.maxpool(conv2)     \n",
    "\n",
    "        conv3 = self.dconv_down3(x)          \n",
    "        x = self.maxpool(conv3)     \n",
    "\n",
    "        x = self.dconv_down4(x)    \n",
    "        x = self.upsample(x)        \n",
    "        # print('La taille de x est ', x.shape, 'et la taille de conv3 est ', conv3.shape)\n",
    "        x = tch.cat([x, conv3], dim=1) \n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        # print('La taille de x est ', x.shape, 'et la taille de conv2 est ', conv2.shape)\n",
    "        x = tch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        #  print('La taille de x est ', x.shape, 'et la taille de conv1 est ', conv3.shape)\n",
    "        x = tch.cat([x, conv1], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        out = self.conv_last(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "unet_model = UNet(in_channels = 3, out_channels = 3)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "\n",
    "# Définir la fonction de perte (criterion) et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = tch.optim.Adam(unet_model.parameters(), lr=args.lr_rate)\n",
    "\n",
    "unet_model.train()\n",
    "\n",
    "\n",
    "for epoch in range(args.n_epoch) :\n",
    "    running_loss = 0.0\n",
    "    for image, mask in data_loader :\n",
    "        # Remettre à zéro les gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = unet_model(image)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(pred, mask)\n",
    "\n",
    "        # Backpropagation and update of the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the whole loss of the epoch\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Afficher la perte moyenne de l'époque\n",
    "    print(f\"Epoch [{epoch+1}/{args.n_epoch}], Loss: {running_loss/len(data_loader)}\")\n",
    "\n",
    "    \n",
    "# Model saving\n",
    "# Check https://pytorch.org/tutorials/beginner/saving_loading_models.html to save correctly the model\n",
    "local_model_path = \"unet_model.pt\"\n",
    "tch.save(unet_model.state_dict(), local_model_path)\n",
    "\n",
    "object_path = 'model/' + local_model_path\n",
    "blob = bucket.blob(object_path)\n",
    "blob.upload_from_filename(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d08307-ff33-4f75-9e0f-fd59b0be7111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_unet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe84932-7051-403f-9dd7-818a8bb5d7d1",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcce238-fdcc-4ff8-86fb-938d5b56fe2f",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e5c262f-482f-40ce-be9b-7b9b4041167e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ob# Modify this for the training\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task_unet_model.py\",\n",
    "    container_uri=\"us-docker.pkg.dev/vertex-ai/training/tf-cpu.2-8:latest\", # here\n",
    "    model_serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-8:latest\", # and here\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af329337-2e17-4a2a-8612-feea0eabe6d3",
   "metadata": {},
   "source": [
    "### Creating and training the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5a5edfe-f975-4a7f-90ac-6afc7cc0a790",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://semantic-segmentation-on-kitti-aip-20240328170341/aiplatform-2024-04-02-15:17:34.288-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://semantic-segmentation-on-kitti-aip-20240328170341/aiplatform-custom-training-2024-04-02-15:17:34.454 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/4010323561137831936?project=549378803954\n",
      "CustomTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/4010323561137831936 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "CustomTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/4010323561137831936 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "CustomTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/4010323561137831936 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "CustomTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/4010323561137831936 current state:\n",
      "PipelineState.PIPELINE_STATE_PENDING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/8163486842503561216?project=549378803954\n",
      "CustomTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/4010323561137831936 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. \\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/runpy.py\\\", line 193, in _run_module_as_main\\n    \\\"__main__\\\", mod_spec)\\n  File \\\"/opt/conda/lib/python3.7/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"/root/.local/lib/python3.7/site-packages/aiplatform_custom_trainer_script/task.py\\\", line 2, in <module>\\n    import torch as tch\\nModuleNotFoundError: No module named \\'torch\\'\\n\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=549378803954&resource=ml_job%2Fjob_id%2F8163486842503561216&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%228163486842503561216%22\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m MODEL_DISPLAY_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munet_model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Start the training and create your model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_DISPLAY_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:3278\u001b[0m, in \u001b[0;36mCustomTrainingJob.run\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, sync, create_request_timeout, disable_retries)\u001b[0m\n\u001b[1;32m   3273\u001b[0m \u001b[38;5;66;03m# make and copy package\u001b[39;00m\n\u001b[1;32m   3274\u001b[0m python_packager \u001b[38;5;241m=\u001b[39m source_utils\u001b[38;5;241m.\u001b[39m_TrainingScriptPythonPackager(\n\u001b[1;32m   3275\u001b[0m     script_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_script_path, requirements\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requirements\n\u001b[1;32m   3276\u001b[0m )\n\u001b[0;32m-> 3278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpython_packager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpython_packager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3282\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworker_pool_specs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworker_pool_specs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanaged_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanaged_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_default_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_default_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3289\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3290\u001b[0m \u001b[43m    \u001b[49m\u001b[43menvironment_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menvironment_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_output_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3294\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbigquery_destination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_destination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrestart_job_on_worker_restart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3305\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_web_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_web_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3306\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_dashboard_access\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_dashboard_access\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensorboard\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensorboard\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_server_container_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction_server_container_uri\u001b[49m\n\u001b[1;32m   3309\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mreduction_server_replica_count\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[1;32m   3310\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3311\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3314\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:4004\u001b[0m, in \u001b[0;36mCustomTrainingJob._run\u001b[0;34m(self, python_packager, dataset, annotation_schema_uri, worker_pool_specs, managed_model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, reduction_server_container_uri, sync, create_request_timeout, block, disable_retries)\u001b[0m\n\u001b[1;32m   3983\u001b[0m             spec[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython_package_spec\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   3984\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value}\n\u001b[1;32m   3985\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m environment_variables\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   3986\u001b[0m             ]\n\u001b[1;32m   3988\u001b[0m (\n\u001b[1;32m   3989\u001b[0m     training_task_inputs,\n\u001b[1;32m   3990\u001b[0m     base_output_dir,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4001\u001b[0m     disable_retries\u001b[38;5;241m=\u001b[39mdisable_retries,\n\u001b[1;32m   4002\u001b[0m )\n\u001b[0;32m-> 4004\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4005\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_task_definition\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefinition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4006\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_task_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_task_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4007\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4008\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mannotation_schema_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4009\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4010\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4011\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_fraction_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_fraction_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4012\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4013\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4014\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtest_filter_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_filter_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4015\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpredefined_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4016\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimestamp_split_column_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4017\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanaged_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4018\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4019\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparent_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparent_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4020\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_default_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_default_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4021\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_aliases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4022\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_version_description\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_description\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4023\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgcs_destination_uri_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_output_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4024\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbigquery_destination\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbigquery_destination\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4025\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcreate_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4027\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:830\u001b[0m, in \u001b[0;36m_TrainingJob._run_job\u001b[0;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, gcs_destination_uri_prefix, bigquery_destination, create_request_timeout, block)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m training_pipeline\n\u001b[1;32m    828\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView Training:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dashboard_uri())\n\u001b[0;32m--> 830\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    833\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    834\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining did not produce a Managed Model returning None. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    835\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_upload_fail_string\n\u001b[1;32m    836\u001b[0m     )\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:918\u001b[0m, in \u001b[0;36m_TrainingJob._get_model\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to get and instantiate the Model to Upload.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \n\u001b[1;32m    910\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;124;03m    RuntimeError: If Training failed.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[0;32m--> 918\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_block_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_failed:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    922\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Pipeline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed. No model available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    923\u001b[0m     )\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:961\u001b[0m, in \u001b[0;36m_TrainingJob._block_until_complete\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_callback()\n\u001b[1;32m    959\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(_JOB_WAIT_TIME)\n\u001b[0;32m--> 961\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_failure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    963\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    965\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mmodel_to_upload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_failed:\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:978\u001b[0m, in \u001b[0;36m_TrainingJob._raise_failure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to raise failure if TrainingPipeline fails.\u001b[39;00m\n\u001b[1;32m    972\u001b[0m \n\u001b[1;32m    973\u001b[0m \u001b[38;5;124;03mRaises:\u001b[39;00m\n\u001b[1;32m    974\u001b[0m \u001b[38;5;124;03m    RuntimeError: If training failed.\u001b[39;00m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m!=\u001b[39m code_pb2\u001b[38;5;241m.\u001b[39mOK:\n\u001b[0;32m--> 978\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Training failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. \\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/runpy.py\\\", line 193, in _run_module_as_main\\n    \\\"__main__\\\", mod_spec)\\n  File \\\"/opt/conda/lib/python3.7/runpy.py\\\", line 85, in _run_code\\n    exec(code, run_globals)\\n  File \\\"/root/.local/lib/python3.7/site-packages/aiplatform_custom_trainer_script/task.py\\\", line 2, in <module>\\n    import torch as tch\\nModuleNotFoundError: No module named \\'torch\\'\\n\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=549378803954&resource=ml_job%2Fjob_id%2F8163486842503561216&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%228163486842503561216%22\"\n"
     ]
    }
   ],
   "source": [
    "MODEL_DISPLAY_NAME = \"unet_model\"\n",
    "\n",
    "# Start the training and create your model\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1854562-89e8-4b4d-9f4e-1f352ee35acb",
   "metadata": {},
   "source": [
    "### Deploying model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62589580-0761-40ab-bc88-12785cace9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/358435915614/locations/us-central1/endpoints/4200646790522863616/operations/37557599217909760\n",
      "Endpoint created. Resource name: projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/358435915614/locations/us-central1/endpoints/4200646790522863616')\n",
      "Deploying model to Endpoint : projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n",
      "Using default machine_type: n1-standard-2\n",
      "Deploy Endpoint model backing LRO: projects/358435915614/locations/us-central1/endpoints/4200646790522863616/operations/2952512458033463296\n",
      "Endpoint model deployed. Resource name: projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"penguins_deployed_unique\"\n",
    "\n",
    "endpoint = model.deploy(deployed_model_display_name=DEPLOYED_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a8894-4a99-441c-922f-1dacbe61665c",
   "metadata": {},
   "source": [
    "## Make a prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c7769-528a-42c5-83dd-c8aa315a1e6a",
   "metadata": {},
   "source": [
    "### Prepare prediction test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdd48195-1776-4556-bb71-1f47d3ab6464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the species column\n",
    "df_for_prediction.pop(LABEL_COLUMN)\n",
    "\n",
    "# Convert data to a Python list\n",
    "test_data_list = df_for_prediction.values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0cb3e1-4b53-4634-b04c-1af1d451d1cc",
   "metadata": {},
   "source": [
    "### Viewing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07dc4b70-3c1c-41f5-ae60-2850e27ecd39",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 36.6, 18.4, 184.0, 3475.0, 0.0],\n",
       " [0.0, 40.9, 18.9, 184.0, 3900.0, 1.0],\n",
       " [0.0, 37.3, 16.8, 192.0, 3000.0, 0.0],\n",
       " [0.0, 49.5, 19.0, 200.0, 3800.0, 1.0],\n",
       " [0.0, 47.0, 17.3, 185.0, 3700.0, 0.0],\n",
       " [0.0, 34.0, 17.1, 185.0, 3400.0, 0.0],\n",
       " [0.0, 50.6, 19.4, 193.0, 3800.0, 1.0],\n",
       " [0.0, 50.8, 18.5, 201.0, 4450.0, 1.0],\n",
       " [0.0, 39.6, 18.1, 186.0, 4450.0, 1.0],\n",
       " [0.0, 41.3, 20.3, 194.0, 3550.0, 1.0],\n",
       " [0.0, 42.5, 16.7, 187.0, 3350.0, 0.0],\n",
       " [0.0, 40.6, 17.2, 187.0, 3475.0, 1.0],\n",
       " [0.0, 45.7, 17.0, 195.0, 3650.0, 0.0],\n",
       " [0.0, 36.0, 17.8, 195.0, 3450.0, 0.0],\n",
       " [0.0, 50.7, 19.7, 203.0, 4050.0, 1.0],\n",
       " [0.0, 42.2, 18.5, 180.0, 3550.0, 0.0],\n",
       " [0.0, 32.1, 15.5, 188.0, 3050.0, 0.0],\n",
       " [0.0, 50.9, 19.1, 196.0, 3550.0, 1.0],\n",
       " [0.0, 49.0, 19.6, 212.0, 4300.0, 1.0],\n",
       " [0.0, 52.2, 18.8, 197.0, 3450.0, 1.0],\n",
       " [0.0, 53.5, 19.9, 205.0, 4500.0, 1.0],\n",
       " [0.0, 52.8, 20.0, 205.0, 4550.0, 1.0],\n",
       " [0.0, 38.9, 18.8, 190.0, 3600.0, 0.0],\n",
       " [0.0, 38.8, 20.0, 190.0, 3950.0, 1.0],\n",
       " [0.0, 46.4, 18.6, 190.0, 3450.0, 0.0],\n",
       " [1.0, 44.0, 13.6, 208.0, 4350.0, 0.0],\n",
       " [1.0, 48.7, 15.7, 208.0, 5350.0, 1.0],\n",
       " [1.0, 49.4, 15.8, 216.0, 4925.0, 1.0],\n",
       " [1.0, 47.7, 15.0, 216.0, 4750.0, 0.0],\n",
       " [1.0, 50.0, 15.9, 224.0, 5350.0, 1.0],\n",
       " [1.0, 47.5, 14.2, 209.0, 4600.0, 0.0],\n",
       " [1.0, 37.6, 19.1, 194.0, 3750.0, 1.0],\n",
       " [1.0, 42.0, 13.5, 210.0, 4150.0, 0.0],\n",
       " [1.0, 46.6, 14.2, 210.0, 4850.0, 0.0],\n",
       " [1.0, 51.3, 14.2, 218.0, 5300.0, 1.0],\n",
       " [1.0, 50.0, 15.2, 218.0, 5700.0, 1.0],\n",
       " [1.0, 50.8, 15.7, 226.0, 5200.0, 1.0],\n",
       " [1.0, 40.5, 17.9, 187.0, 3200.0, 0.0],\n",
       " [1.0, 46.1, 13.2, 211.0, 4500.0, 0.0],\n",
       " [1.0, 44.4, 17.3, 219.0, 5250.0, 1.0],\n",
       " [1.0, 41.1, 19.1, 188.0, 4100.0, 1.0],\n",
       " [1.0, 45.5, 14.5, 212.0, 4750.0, 0.0],\n",
       " [1.0, 48.4, 16.3, 220.0, 5400.0, 1.0],\n",
       " [1.0, 50.0, 15.3, 220.0, 5550.0, 1.0],\n",
       " [1.0, 48.5, 14.1, 220.0, 5300.0, 1.0],\n",
       " [1.0, 42.2, 19.5, 197.0, 4275.0, 1.0],\n",
       " [1.0, 42.6, 13.7, 213.0, 4950.0, 0.0],\n",
       " [1.0, 48.2, 15.6, 221.0, 5100.0, 1.0],\n",
       " [1.0, 49.5, 16.2, 229.0, 5800.0, 1.0],\n",
       " [1.0, 38.1, 16.5, 198.0, 3825.0, 0.0],\n",
       " [1.0, 48.8, 16.2, 222.0, 6000.0, 1.0],\n",
       " [1.0, 48.6, 16.0, 230.0, 5800.0, 1.0],\n",
       " [1.0, 45.1, 14.5, 207.0, 5050.0, 0.0],\n",
       " [1.0, 45.1, 14.5, 215.0, 5000.0, 0.0],\n",
       " [1.0, 47.6, 14.5, 215.0, 5400.0, 1.0],\n",
       " [1.0, 46.8, 16.1, 215.0, 5500.0, 1.0],\n",
       " [1.0, 45.2, 15.8, 215.0, 5300.0, 1.0],\n",
       " [1.0, 46.1, 15.1, 215.0, 5100.0, 1.0],\n",
       " [1.0, 50.7, 15.0, 223.0, 5550.0, 1.0],\n",
       " [2.0, 34.4, 18.4, 184.0, 3325.0, 0.0],\n",
       " [2.0, 35.1, 19.4, 193.0, 4200.0, 1.0],\n",
       " [2.0, 46.0, 21.5, 194.0, 4200.0, 1.0],\n",
       " [2.0, 36.2, 17.2, 187.0, 3150.0, 0.0],\n",
       " [2.0, 45.8, 18.9, 197.0, 4150.0, 1.0],\n",
       " [2.0, 38.5, 17.9, 190.0, 3325.0, 0.0],\n",
       " [2.0, 38.8, 17.6, 191.0, 3275.0, 0.0],\n",
       " [2.0, 40.6, 19.0, 199.0, 4000.0, 1.0]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82254a88-e846-4280-9e2b-fa1c13fdf0c3",
   "metadata": {},
   "source": [
    "### Send the prediction request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f66d8a4e-6c9c-414d-a582-76a460760972",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.380821019, 0.225905389, 0.393273532],\n",
       " [0.349367857, 0.209143579, 0.441488594],\n",
       " [0.411382914, 0.242850572, 0.345766455],\n",
       " [0.365590334, 0.216427699, 0.417981952],\n",
       " [0.359883547, 0.219310671, 0.420805812],\n",
       " [0.386932313, 0.227012664, 0.386055022],\n",
       " [0.359957278, 0.217440203, 0.422602445],\n",
       " [0.317476153, 0.187540948, 0.494982898],\n",
       " [0.310005456, 0.183340982, 0.506653547],\n",
       " [0.382173508, 0.224923551, 0.392902941],\n",
       " [0.386478394, 0.231975809, 0.381545722],\n",
       " [0.380858064, 0.226311743, 0.392830223],\n",
       " [0.371661782, 0.220337793, 0.40800038],\n",
       " [0.39031291, 0.225384504, 0.384302586],\n",
       " [0.34987697, 0.206286147, 0.443836898],\n",
       " [0.369881153, 0.225210309, 0.404908508],\n",
       " [0.409266233, 0.238573581, 0.352160156],\n",
       " [0.377004445, 0.227536932, 0.395458639],\n",
       " [0.338987589, 0.194405153, 0.466607243],\n",
       " [0.382138848, 0.231711939, 0.386149198],\n",
       " [0.317140281, 0.187296942, 0.495562822],\n",
       " [0.313786119, 0.184895352, 0.501318514],\n",
       " [0.376017094, 0.221398488, 0.402584374],\n",
       " [0.352369934, 0.206819162, 0.440810919],\n",
       " [0.381120563, 0.230365857, 0.388513535],\n",
       " [0.328546554, 0.185095683, 0.486357689],\n",
       " [0.255289406, 0.145119831, 0.599590778],\n",
       " [0.291785926, 0.163462818, 0.544751227],\n",
       " [0.304279, 0.169987619, 0.525733352],\n",
       " [0.266194731, 0.146314576, 0.587490797],\n",
       " [0.309527099, 0.17569302, 0.514779866],\n",
       " [0.370519698, 0.212848768, 0.41663155],\n",
       " [0.345914245, 0.192970812, 0.461114973],\n",
       " [0.292188853, 0.164812967, 0.542998195],\n",
       " [0.263584822, 0.147330731, 0.589084387],\n",
       " [0.236496657, 0.131535679, 0.631967604],\n",
       " [0.278121948, 0.152602166, 0.569275856],\n",
       " [0.397179514, 0.236307412, 0.366513044],\n",
       " [0.318334937, 0.179013, 0.502652109],\n",
       " [0.273509532, 0.149836779, 0.576653719],\n",
       " [0.338504404, 0.198974758, 0.462520778],\n",
       " [0.301867843, 0.16903919, 0.529092968],\n",
       " [0.26073578, 0.143994108, 0.595270097],\n",
       " [0.248492584, 0.137670174, 0.613837242],\n",
       " [0.266120315, 0.146793008, 0.587086678],\n",
       " [0.332410336, 0.191828668, 0.475761026],\n",
       " [0.288427651, 0.159462482, 0.552109838],\n",
       " [0.282916218, 0.155910984, 0.561172843],\n",
       " [0.237874895, 0.128774494, 0.63335067],\n",
       " [0.364900947, 0.207851902, 0.427247196],\n",
       " [0.219762683, 0.12057703, 0.65966028],\n",
       " [0.238738462, 0.128565922, 0.632695556],\n",
       " [0.27621156, 0.156089947, 0.567698479],\n",
       " [0.285746396, 0.1584993, 0.555754304],\n",
       " [0.256069958, 0.142519832, 0.601410151],\n",
       " [0.250563085, 0.139251292, 0.610185623],\n",
       " [0.265381366, 0.146881357, 0.587737262],\n",
       " [0.279032707, 0.154909179, 0.566058099],\n",
       " [0.250029117, 0.137798548, 0.612172306],\n",
       " [0.393622547, 0.228289008, 0.378088415],\n",
       " [0.339176148, 0.191811264, 0.469012648],\n",
       " [0.3366808, 0.196669444, 0.466649741],\n",
       " [0.403329611, 0.234582, 0.362088412],\n",
       " [0.340265065, 0.196822762, 0.462912142],\n",
       " [0.394503504, 0.228850499, 0.376646072],\n",
       " [0.397529781, 0.230581194, 0.371889085],\n",
       " [0.355459839, 0.201628268, 0.442911893]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get your predictions.\n",
    "predictions = endpoint.predict(instances=test_data_list)\n",
    "\n",
    "# View the predictions\n",
    "predictions.predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fee8e398-8708-4a75-a154-c640ce79790b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 0, 2, 2, 0, 2, 2, 0, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 2, 0, 0,\n",
       "       2])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the prediction for each set of input data.\n",
    "species_predictions = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# View the best prediction for the penguin characteristics in each row.\n",
    "species_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ccbb82d-aa34-4d22-a39d-feccd14de4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting CustomTrainingJob : projects/358435915614/locations/us-central1/trainingPipelines/4620196032267943936\n",
      "Delete CustomTrainingJob  backing LRO: projects/358435915614/locations/us-central1/operations/8096928352792739840\n",
      "CustomTrainingJob deleted. . Resource name: projects/358435915614/locations/us-central1/trainingPipelines/4620196032267943936\n",
      "Undeploying Endpoint model: projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n",
      "Undeploy Endpoint model backing LRO: projects/358435915614/locations/us-central1/endpoints/4200646790522863616/operations/7745647581857841152\n",
      "Endpoint model undeployed. Resource name: projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n",
      "Deleting Endpoint : projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n",
      "Delete Endpoint  backing LRO: projects/358435915614/locations/us-central1/operations/3642868321323319296\n",
      "Endpoint deleted. . Resource name: projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n",
      "Deleting Model : projects/358435915614/locations/us-central1/models/6078159651973627904\n",
      "Delete Model  backing LRO: projects/358435915614/locations/us-central1/models/6078159651973627904/operations/7772669179622064128\n",
      "Model deleted. . Resource name: projects/358435915614/locations/us-central1/models/6078159651973627904\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the endpoint and undeploy the model from it\n",
    "endpoint.delete(force=True)\n",
    "\n",
    "# Delete the model\n",
    "model.delete()\n",
    "\n",
    "# Delete the storage bucket and its contents\n",
    "bucket.delete(force=True)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cnn-py3.10",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "cnn-py3.10 (Local)",
   "language": "python",
   "name": "cnn-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
