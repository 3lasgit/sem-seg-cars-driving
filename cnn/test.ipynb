{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da5893a-2e5e-48b2-9ef1-e573611d54fb",
   "metadata": {},
   "source": [
    "## Prerequesites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03fd7cb-9083-40e5-858d-54aa2bb162fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as tch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54299d6-b08c-4f4e-a1da-0e8cd49c89d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "\n",
      "\n",
      "To take a quick anonymous survey, run:\n",
      "  $ gcloud survey\n",
      "\n"
     ]
    }
   ],
   "source": [
    "project_id = \"semantic-segmentation-on-kitti\" # @param {type:\"string\"}\n",
    "# Set the project id\n",
    "! gcloud config set project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538bb6b8-1fb4-48d0-90af-de284d6b8488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "125b7be0-d446-4632-b990-2a9eace3d50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://data_kitti_driv_seg\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'data_kitti_driv_seg' # @param {type:\"string\"}\n",
    "bucket_uri = f\"gs://{bucket_name}\"\n",
    "\n",
    "# bucket_name = \"semantic-segmentation-on-kitti-aip-20240328170341\"\n",
    "# bucket_uri = \"gs://semantic-segmentation-on-kitti-aip-20240328170341\"\n",
    "\n",
    "# from datetime import datetime\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# if bucket_name == \"\" or bucket_name is None or bucket_name == \"bucket-name-placeholder\":\n",
    "#     bucket_name = project_id + \"-aip-\" + timestamp\n",
    "#     bucket_uri = \"gs://\" + bucket_name\n",
    "! echo $bucket_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1103f9f-371e-47aa-96a2-b6a78b5304ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client(project=project_id)\n",
    "\n",
    "# Create a bucket\n",
    "bucket = client.get_bucket(bucket_name) # client.create_bucket(bucket_name, location=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7473a1ad-3243-4a40-8e04-f38ee870909d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket data_kitti_driv_seg loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Bucket {} loaded.\".format(bucket.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f89c3cf9-03ca-4ef4-8b10-b56bc8866737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=project_id, location=region, staging_bucket=bucket_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7785da-6ad7-45be-8e7c-6a700b839e5d",
   "metadata": {},
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3ead584-cc21-46f4-9676-273496b0c6fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task_unet_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task_unet_model.py\n",
    "\n",
    "import torch as tch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Importing the data \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr_rate', default=0.001, type=int)\n",
    "parser.add_argument('--n_epoch', default=50, type=int)\n",
    "parser.add_argument('--batch_size', default=1, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Add saving and loading models for big epochs' training\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# Initialiser le client GCS\n",
    "\n",
    "#project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "project_id = \"semantic-segmentation-on-kitti\"\n",
    "client = storage.Client(project=project_id)\n",
    "\n",
    "\n",
    "bucket_name = 'data_kitti_driv_seg'\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    \n",
    "from io import BytesIO\n",
    "\n",
    "# Récupérer l'objet depuis le bucket\n",
    "object_path = 'data/training_tensor.pt'\n",
    "blob = bucket.blob(object_path)\n",
    "# Télécharger les données de l'objet en mémoire\n",
    "data = BytesIO(blob.download_as_string())\n",
    "\n",
    "training_tensor = tch.load(data)\n",
    "training_tensor.shape\n",
    "\n",
    "\n",
    "# Constructing the dataset objects\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, data_tensor):\n",
    "        self.data = data_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]  # Nombre d'exemples dans le tensor data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Extraire l'image et le masque correspondant à l'index donné\n",
    "        image = self.data[0, index]  # Première dimension pour les images\n",
    "        mask = self.data[1, index]   # Deuxième dimension pour les masques\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "# Splitting data into training/test datasets\n",
    "\n",
    "training_data, test_data = ImageMaskDataset(training_tensor[:,:160]), ImageMaskDataset(training_tensor[:,160:])\n",
    "\n",
    "# Création du DataLoader\n",
    "data_loader = tch.utils.data.DataLoader(training_data, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Building the CNN (U-Net)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module): # Creating a class merging the double conv\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),          # X_out=X_in cf formula applied with these parameters' values\n",
    "            nn.BatchNorm2d(out_channels),                                                # keeps size\n",
    "            nn.ReLU(inplace=True),                                                       # keeps size \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),         \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                                                                # Keeps the same image size of the input\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.dconv_down1 = DoubleConv(in_channels, 64)        # keeps image size \n",
    "        self.dconv_down2 = DoubleConv(64, 128)                # keeps image size \n",
    "        self.dconv_down3 = DoubleConv(128, 256)               # keeps image size \n",
    "        self.dconv_down4 = DoubleConv(256, 512)               # keeps image size \n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2)          # X_out=int((X_in/2) + 1)   # Caution : default stride is equal to kernel-size here\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # X_out=int(X_in*2)       # Fasten the process as it hasn't to learn weights unlike the convtranspose (which is so )\n",
    "        \n",
    "        self.dconv_up3 = DoubleConv(256 + 512, 256)          # keeps image size \n",
    "        self.dconv_up2 = DoubleConv(128 + 256, 128)          # keeps image size\n",
    "        self.dconv_up1 = DoubleConv(128 + 64, 64)            # keeps image size\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)      # keeps image size\n",
    "\n",
    "    def forward(self, x): \n",
    "        conv1 = self.dconv_down1(x)          \n",
    "        x = self.maxpool(conv1)     \n",
    "\n",
    "        conv2 = self.dconv_down2(x)          \n",
    "        x = self.maxpool(conv2)     \n",
    "\n",
    "        conv3 = self.dconv_down3(x)          \n",
    "        x = self.maxpool(conv3)     \n",
    "\n",
    "        x = self.dconv_down4(x)    \n",
    "        x = self.upsample(x)        \n",
    "        # print('La taille de x est ', x.shape, 'et la taille de conv3 est ', conv3.shape)\n",
    "        x = tch.cat([x, conv3], dim=1) \n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        # print('La taille de x est ', x.shape, 'et la taille de conv2 est ', conv2.shape)\n",
    "        x = tch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        #  print('La taille de x est ', x.shape, 'et la taille de conv1 est ', conv3.shape)\n",
    "        x = tch.cat([x, conv1], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        out = self.conv_last(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "unet_model = UNet(in_channels = 3, out_channels = 3)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "\n",
    "# Définir la fonction de perte (criterion) et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = tch.optim.Adam(unet_model.parameters(), lr=args.lr_rate)\n",
    "\n",
    "# Add saving method for optimisizer and epoch as said here https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
    "\n",
    "unet_model.train()\n",
    "\n",
    "\n",
    "for epoch in range(args.n_epoch) :\n",
    "    running_loss = 0.0 \n",
    "    \n",
    "    if epoch % 5 == 0 :\n",
    "        local_model_path = \"unet_model.pt\"\n",
    "        object_path = 'model/' + local_model_path\n",
    "        blob = bucket.blob(object_path)\n",
    "        \n",
    "        # Download the model locally\n",
    "        blob.download_to_filename(local_model_path)\n",
    "\n",
    "        # Charger les poids du modèle depuis le fichier\n",
    "        state_dict = tch.load(local_model_path)\n",
    "\n",
    "        # Mettre à jour les paramètres du modèle avec les poids chargés\n",
    "        unet_model.load_state_dict(state_dict)\n",
    "        unet_model.train()\n",
    "        \n",
    "    for image, mask in data_loader :\n",
    "        # Remettre à zéro les gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = unet_model(image)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(pred, mask)\n",
    "\n",
    "        # Backpropagation and update of the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the whole loss of the epoch\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if epoch % 5 == 0 :\n",
    "        local_model_path = \"unet_model.pt\"\n",
    "        tch.save(unet_model.state_dict(), local_model_path)\n",
    "\n",
    "        object_path = 'model/' + local_model_path\n",
    "        blob = bucket.blob(object_path)\n",
    "        blob.upload_from_filename(local_model_path)\n",
    "\n",
    "    # Afficher la perte moyenne de l'époque\n",
    "    print(f\"Epoch [{epoch+1}/{args.n_epoch}], Loss: {running_loss/len(data_loader)}\")\n",
    "\n",
    "    \n",
    "# Model saving\n",
    "\n",
    "local_model_path = \"unet_model.pt\"\n",
    "tch.save(unet_model.state_dict(), local_model_path)\n",
    "\n",
    "object_path = 'model/' + local_model_path\n",
    "blob = bucket.blob(object_path)\n",
    "blob.upload_from_filename(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57d08307-ff33-4f75-9e0f-fd59b0be7111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_unet\"\n",
    "container_uri = \"us-central1-docker.pkg.dev/semantic-segmentation-on-kitti/unet-model-train-job/unet-model:tag1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe84932-7051-403f-9dd7-818a8bb5d7d1",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcce238-fdcc-4ff8-86fb-938d5b56fe2f",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75775317-ca0b-457c-b5f6-4b6ea5c5c36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=container_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "713b6696-181a-4441-a8e8-7a2e423448c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DISPLAY_NAME = \"unet_model\"\n",
    "args = [\"--epochs\", \"1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a099360-eca7-4a5c-8e9f-d7fb92aff2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "model_display_name was provided but\n                model_serving_container_image_uri was not provided when this\n                custom pipeline was constructed.\n                ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMODEL_DISPLAY_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# tensorboard=tensorboard_resource_name,\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m model\u001b[38;5;241m.\u001b[39mwait()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(model\u001b[38;5;241m.\u001b[39mdisplay_name)\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:4617\u001b[0m, in \u001b[0;36mCustomContainerTrainingJob.run\u001b[0;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, sync, create_request_timeout, disable_retries)\u001b[0m\n\u001b[1;32m   4614\u001b[0m network \u001b[38;5;241m=\u001b[39m network \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mnetwork\n\u001b[1;32m   4615\u001b[0m service_account \u001b[38;5;241m=\u001b[39m service_account \u001b[38;5;129;01mor\u001b[39;00m initializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mservice_account\n\u001b[0;32m-> 4617\u001b[0m worker_pool_specs, managed_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_and_validate_run\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4618\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreplica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4622\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4623\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboot_disk_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboot_disk_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mboot_disk_size_gb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboot_disk_size_gb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_server_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction_server_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreduction_server_machine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduction_server_machine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4630\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\n\u001b[1;32m   4631\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[1;32m   4632\u001b[0m     annotation_schema_uri\u001b[38;5;241m=\u001b[39mannotation_schema_uri,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4664\u001b[0m     disable_retries\u001b[38;5;241m=\u001b[39mdisable_retries,\n\u001b[1;32m   4665\u001b[0m )\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/google/cloud/aiplatform/training_jobs.py:1436\u001b[0m, in \u001b[0;36m_CustomTrainingJob._prepare_and_validate_run\u001b[0;34m(self, model_display_name, model_labels, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type)\u001b[0m\n\u001b[1;32m   1434\u001b[0m \u001b[38;5;66;03m# if args needed for model is incomplete\u001b[39;00m\n\u001b[1;32m   1435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_display_name \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_model\u001b[38;5;241m.\u001b[39mcontainer_spec\u001b[38;5;241m.\u001b[39mimage_uri:\n\u001b[0;32m-> 1436\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1437\u001b[0m \u001b[38;5;250m        \u001b[39m\u001b[38;5;124;03m\"\"\"model_display_name was provided but\u001b[39;00m\n\u001b[1;32m   1438\u001b[0m \u001b[38;5;124;03m        model_serving_container_image_uri was not provided when this\u001b[39;00m\n\u001b[1;32m   1439\u001b[0m \u001b[38;5;124;03m        custom pipeline was constructed.\u001b[39;00m\n\u001b[1;32m   1440\u001b[0m \u001b[38;5;124;03m        \"\"\"\u001b[39;00m\n\u001b[1;32m   1441\u001b[0m     )\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_managed_model\u001b[38;5;241m.\u001b[39mcontainer_spec\u001b[38;5;241m.\u001b[39mimage_uri:\n\u001b[1;32m   1444\u001b[0m     model_display_name \u001b[38;5;241m=\u001b[39m model_display_name \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_display_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: model_display_name was provided but\n                model_serving_container_image_uri was not provided when this\n                custom pipeline was constructed.\n                "
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    args=args,\n",
    "    # tensorboard=tensorboard_resource_name,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62589580-0761-40ab-bc88-12785cace9a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/358435915614/locations/us-central1/endpoints/4200646790522863616/operations/37557599217909760\n",
      "Endpoint created. Resource name: projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/358435915614/locations/us-central1/endpoints/4200646790522863616')\n",
      "Deploying model to Endpoint : projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n",
      "Using default machine_type: n1-standard-2\n",
      "Deploy Endpoint model backing LRO: projects/358435915614/locations/us-central1/endpoints/4200646790522863616/operations/2952512458033463296\n",
      "Endpoint model deployed. Resource name: projects/358435915614/locations/us-central1/endpoints/4200646790522863616\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"penguins_deployed_unique\"\n",
    "\n",
    "endpoint = model.deploy(deployed_model_display_name=DEPLOYED_NAME)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cnn-py3.10",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "cnn-py3.10 (Local)",
   "language": "python",
   "name": "cnn-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
