{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6da5893a-2e5e-48b2-9ef1-e573611d54fb",
   "metadata": {},
   "source": [
    "## Prerequesites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d03fd7cb-9083-40e5-858d-54aa2bb162fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as tch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54299d6-b08c-4f4e-a1da-0e8cd49c89d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "project_id = \"semantic-segmentation-on-kitti\" # @param {type:\"string\"}\n",
    "# Set the project id\n",
    "! gcloud config set project {project_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "538bb6b8-1fb4-48d0-90af-de284d6b8488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "125b7be0-d446-4632-b990-2a9eace3d50a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://kitti_driv_seg_unet\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'kitti_driv_seg_unet' # @param {type:\"string\"}\n",
    "bucket_uri = f\"gs://{bucket_name}\"\n",
    "\n",
    "# bucket_name = \"semantic-segmentation-on-kitti-aip-20240328170341\"\n",
    "# bucket_uri = \"gs://semantic-segmentation-on-kitti-aip-20240328170341\"\n",
    "\n",
    "# from datetime import datetime\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "\n",
    "# if bucket_name == \"\" or bucket_name is None or bucket_name == \"bucket-name-placeholder\":\n",
    "#     bucket_name = project_id + \"-aip-\" + timestamp\n",
    "#     bucket_uri = \"gs://\" + bucket_name\n",
    "! echo $bucket_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1103f9f-371e-47aa-96a2-b6a78b5304ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "client = storage.Client(project=project_id)\n",
    "\n",
    "# Create a bucket\n",
    "bucket = client.get_bucket(bucket_name) # client.create_bucket(bucket_name, location=region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7473a1ad-3243-4a40-8e04-f38ee870909d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket kitti_driv_seg_unet loaded.\n"
     ]
    }
   ],
   "source": [
    "print(\"Bucket {} loaded.\".format(bucket.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f89c3cf9-03ca-4ef4-8b10-b56bc8866737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize the Vertex AI SDK\n",
    "aiplatform.init(project=project_id, location=region, staging_bucket=bucket_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7785da-6ad7-45be-8e7c-6a700b839e5d",
   "metadata": {},
   "source": [
    "## Training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c3ead584-cc21-46f4-9676-273496b0c6fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task_unet_model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task_unet_model.py\n",
    "\n",
    "import torch as tch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "# Importing the data \n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr_rate', default=0.001, type=int)\n",
    "parser.add_argument('--n_epoch', default=50, type=int)\n",
    "parser.add_argument('--batch_size', default=1, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "\n",
    "# Add saving and loading models for big epochs' training\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# Initialiser le client GCS\n",
    "\n",
    "#project_number = os.environ[\"CLOUD_ML_PROJECT_ID\"]\n",
    "project_id = \"semantic-segmentation-on-kitti\"\n",
    "client = storage.Client(project=project_id)\n",
    "\n",
    "\n",
    "bucket_name = 'data_kitti_driv_seg'\n",
    "bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    \n",
    "from io import BytesIO\n",
    "\n",
    "# Récupérer l'objet depuis le bucket\n",
    "object_path = 'data/training_tensor.pt'\n",
    "blob = bucket.blob(object_path)\n",
    "# Télécharger les données de l'objet en mémoire\n",
    "data = BytesIO(blob.download_as_string())\n",
    "\n",
    "training_tensor = tch.load(data)\n",
    "training_tensor.shape\n",
    "\n",
    "\n",
    "# Constructing the dataset objects\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, data_tensor):\n",
    "        self.data = data_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]  # Nombre d'exemples dans le tensor data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Extraire l'image et le masque correspondant à l'index donné\n",
    "        image = self.data[0, index]  # Première dimension pour les images\n",
    "        mask = self.data[1, index]   # Deuxième dimension pour les masques\n",
    "        \n",
    "        return image, mask\n",
    "    \n",
    "# Splitting data into training/test datasets\n",
    "\n",
    "training_data, test_data = ImageMaskDataset(training_tensor[:,:160]), ImageMaskDataset(training_tensor[:,160:])\n",
    "\n",
    "# Création du DataLoader\n",
    "data_loader = tch.utils.data.DataLoader(training_data, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "# Building the CNN (U-Net)\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module): # Creating a class merging the double conv\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),          # X_out=X_in cf formula applied with these parameters' values\n",
    "            nn.BatchNorm2d(out_channels),                                                # keeps size\n",
    "            nn.ReLU(inplace=True),                                                       # keeps size \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),         \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                                                                # Keeps the same image size of the input\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.dconv_down1 = DoubleConv(in_channels, 64)        # keeps image size \n",
    "        self.dconv_down2 = DoubleConv(64, 128)                # keeps image size \n",
    "        self.dconv_down3 = DoubleConv(128, 256)               # keeps image size \n",
    "        self.dconv_down4 = DoubleConv(256, 512)               # keeps image size \n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2)          # X_out=int((X_in/2) + 1)   # Caution : default stride is equal to kernel-size here\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # X_out=int(X_in*2)       # Fasten the process as it hasn't to learn weights unlike the convtranspose (which is so )\n",
    "        \n",
    "        self.dconv_up3 = DoubleConv(256 + 512, 256)          # keeps image size \n",
    "        self.dconv_up2 = DoubleConv(128 + 256, 128)          # keeps image size\n",
    "        self.dconv_up1 = DoubleConv(128 + 64, 64)            # keeps image size\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)      # keeps image size\n",
    "\n",
    "    def forward(self, x): \n",
    "        conv1 = self.dconv_down1(x)          \n",
    "        x = self.maxpool(conv1)     \n",
    "\n",
    "        conv2 = self.dconv_down2(x)          \n",
    "        x = self.maxpool(conv2)     \n",
    "\n",
    "        conv3 = self.dconv_down3(x)          \n",
    "        x = self.maxpool(conv3)     \n",
    "\n",
    "        x = self.dconv_down4(x)    \n",
    "        x = self.upsample(x)        \n",
    "        # print('La taille de x est ', x.shape, 'et la taille de conv3 est ', conv3.shape)\n",
    "        x = tch.cat([x, conv3], dim=1) \n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        # print('La taille de x est ', x.shape, 'et la taille de conv2 est ', conv2.shape)\n",
    "        x = tch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        #  print('La taille de x est ', x.shape, 'et la taille de conv1 est ', conv3.shape)\n",
    "        x = tch.cat([x, conv1], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        out = self.conv_last(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "unet_model = UNet(in_channels = 3, out_channels = 3)\n",
    "\n",
    "\n",
    "# Training the model\n",
    "\n",
    "# Définir la fonction de perte (criterion) et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = tch.optim.Adam(unet_model.parameters(), lr=args.lr_rate)\n",
    "\n",
    "# Add saving method for optimisizer and epoch as said here https://pytorch.org/tutorials/recipes/recipes/saving_and_loading_a_general_checkpoint.html\n",
    "\n",
    "unet_model.train()\n",
    "\n",
    "\n",
    "for epoch in range(args.n_epoch) :\n",
    "    running_loss = 0.0 \n",
    "    \n",
    "    if epoch % 5 == 0 :\n",
    "        local_model_path = \"unet_model.pt\"\n",
    "        object_path = 'model/' + local_model_path\n",
    "        blob = bucket.blob(object_path)\n",
    "        \n",
    "        # Download the model locally\n",
    "        blob.download_to_filename(local_model_path)\n",
    "\n",
    "        # Charger les poids du modèle depuis le fichier\n",
    "        state_dict = tch.load(local_model_path)\n",
    "\n",
    "        # Mettre à jour les paramètres du modèle avec les poids chargés\n",
    "        unet_model.load_state_dict(state_dict)\n",
    "        unet_model.train()\n",
    "        \n",
    "    for image, mask in data_loader :\n",
    "        # Remettre à zéro les gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = unet_model(image)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(pred, mask)\n",
    "\n",
    "        # Backpropagation and update of the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the whole loss of the epoch\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    if epoch % 5 == 0 :\n",
    "        local_model_path = \"unet_model.pt\"\n",
    "        tch.save(unet_model.state_dict(), local_model_path)\n",
    "\n",
    "        object_path = 'model/' + local_model_path\n",
    "        blob = bucket.blob(object_path)\n",
    "        blob.upload_from_filename(local_model_path)\n",
    "\n",
    "    # Afficher la perte moyenne de l'époque\n",
    "    print(f\"Epoch [{epoch+1}/{args.n_epoch}], Loss: {running_loss/len(data_loader)}\")\n",
    "\n",
    "    \n",
    "# Model saving\n",
    "\n",
    "local_model_path = \"unet_model.pt\"\n",
    "tch.save(unet_model.state_dict(), local_model_path)\n",
    "\n",
    "object_path = 'model/' + local_model_path\n",
    "blob = bucket.blob(object_path)\n",
    "blob.upload_from_filename(local_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57d08307-ff33-4f75-9e0f-fd59b0be7111",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_unet\"\n",
    "container_uri = \"us-central1-docker.pkg.dev/semantic-segmentation-on-kitti/unet-model-train-job/unet-model:tag1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe84932-7051-403f-9dd7-818a8bb5d7d1",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcce238-fdcc-4ff8-86fb-938d5b56fe2f",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "75775317-ca0b-457c-b5f6-4b6ea5c5c36d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.CustomContainerTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    container_uri=container_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "713b6696-181a-4441-a8e8-7a2e423448c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# MODEL_DISPLAY_NAME = \"unet_model\"\n",
    "args = [\"--n_epoch\", \"50\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a099360-eca7-4a5c-8e9f-d7fb92aff2b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Output directory:\n",
      "gs://kitti_driv_seg_unet/aiplatform-custom-training-2024-05-15-12:55:29.160 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/193502535976222720?project=549378803954\n",
      "CustomContainerTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/193502535976222720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5591928706495938560?project=549378803954\n",
      "CustomContainerTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/193502535976222720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/193502535976222720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/193502535976222720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/193502535976222720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/193502535976222720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomContainerTrainingJob projects/549378803954/locations/us-central1/trainingPipelines/193502535976222720 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n"
     ]
    }
   ],
   "source": [
    "model = job.run(\n",
    "    args=args,\n",
    "    machine_type=\"n2-standard-8\",\n",
    "    # tensorboard=tensorboard_resource_name,\n",
    ")\n",
    "\n",
    "# model.wait()\n",
    "\n",
    "# print(model.uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62589580-0761-40ab-bc88-12785cace9a9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'deploy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m DEPLOYED_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpenguins_deployed_unique\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m(deployed_model_display_name\u001b[38;5;241m=\u001b[39mDEPLOYED_NAME)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'deploy'"
     ]
    }
   ],
   "source": [
    "# DEPLOYED_NAME = \"penguins_deployed_unique\"\n",
    "\n",
    "# endpoint = model.deploy(deployed_model_display_name=DEPLOYED_NAME)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cnn-py3.10",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": "cnn-py3.10 (Local)",
   "language": "python",
   "name": "cnn-py3.10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
