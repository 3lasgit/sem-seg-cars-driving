{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Segmantation for cars driving data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as tch\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/jupyter/sem_seg_cars_driving/cnn/data/training_dataset.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loading Datasets\u001b[39;00m\n\u001b[1;32m      3\u001b[0m path_training, path_test \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mpwd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_dataset.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, Path(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mpwd\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mresolve()\u001b[38;5;241m.\u001b[39mparent \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_dataset.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 5\u001b[0m training_data, test_data \u001b[38;5;241m=\u001b[39m \u001b[43mtch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_training\u001b[49m\u001b[43m)\u001b[49m, tch\u001b[38;5;241m.\u001b[39mload(path_test)\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/torch/serialization.py:998\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    996\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 998\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m   1000\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m   1001\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m   1002\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/torch/serialization.py:445\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 445\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/sem_seg_cars_driving/.venv/lib/python3.10/site-packages/torch/serialization.py:426\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/jupyter/sem_seg_cars_driving/cnn/data/training_dataset.pt'"
     ]
    }
   ],
   "source": [
    "# Loading tensor\n",
    "\n",
    "path_tensor = Path(\"%pwd\").resolve().parent / \"data\" / \"training_tensor.pt\"\n",
    "\n",
    "training_tensor = tch.load(path_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ImageMaskDataset(Dataset):\n",
    "    def __init__(self, data_tensor):\n",
    "        self.data = data_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[1]  # Nombre d'exemples dans le tensor data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Extraire l'image et le masque correspondant à l'index donné\n",
    "        image = self.data[0, index]  # Première dimension pour les images\n",
    "        mask = self.data[1, index]   # Deuxième dimension pour les masques\n",
    "        \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training/test datasets\n",
    "\n",
    "training_data, test_data = ImageMaskDataset(training_tensor[:,:160]), ImageMaskDataset(training_tensor[:,160:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Shows if everything is fine in test_data\n",
    "\n",
    "# ind = np.random.randint(0, 40, 2)\n",
    "# fig = go.Figure()\n",
    "# c=0\n",
    "# for i in ind :\n",
    "#     # Transpose back the images to the good dimensions\n",
    "#     image, mask = test_data[i]\n",
    "#     fig.add_trace(go.Image(z=np.concatenate((image.permute(1, 2, 0)*255, mask.permute(1, 2, 0)*255),1), y0 = c*h_size))\n",
    "# c+=1\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the model will be a custom U-Net, the batch will be one image at a time and we will not work on patches as it is said in their article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du DataLoader\n",
    "batch_size = 1\n",
    "data_loader = tch.utils.data.DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Affichage de la taille du DataLoader, number of batch in our dataloader\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a custom U-Net Model. We decided to suppress the last couple of layer composed of a maxpool, a double-down convolution, an upsampling, a stacking, a double-up convolution layers. We adapted a bit the hyperparameters when needed in order to make everything work back together. \n",
    "\n",
    "We hope that it accelerates the computing without loosing too much accuracy. We may increase the number of channels through the model as we saved some layers. We will see that in the fine-tuning part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class DoubleConv(nn.Module): # Creating a class merging the double conv\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size = 3, padding = 1),          # X_out=X_in cf formula applied with these parameters' values\n",
    "            nn.BatchNorm2d(out_channels),                                                # keeps size\n",
    "            nn.ReLU(inplace=True),                                                       # keeps size \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size = 3, padding = 1),         \n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )                                                                                # Keeps the same image size of the input\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(UNet, self).__init__()\n",
    "        self.dconv_down1 = DoubleConv(in_channels, 64)        # keeps image size \n",
    "        self.dconv_down2 = DoubleConv(64, 128)                # keeps image size \n",
    "        self.dconv_down3 = DoubleConv(128, 256)               # keeps image size \n",
    "        self.dconv_down4 = DoubleConv(256, 512)               # keeps image size \n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2)          # X_out=int((X_in/2) + 1)   # Caution : default stride is equal to kernel-size here\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)  # X_out=int(X_in*2)       # Fasten the process as it hasn't to learn weights unlike the convtranspose (which is so )\n",
    "        \n",
    "        self.dconv_up3 = DoubleConv(256 + 512, 256)          # keeps image size \n",
    "        self.dconv_up2 = DoubleConv(128 + 256, 128)          # keeps image size\n",
    "        self.dconv_up1 = DoubleConv(128 + 64, 64)            # keeps image size\n",
    "\n",
    "        self.conv_last = nn.Conv2d(64, out_channels, 1)      # keeps image size\n",
    "\n",
    "    def forward(self, x): \n",
    "        conv1 = self.dconv_down1(x)          \n",
    "        x = self.maxpool(conv1)     \n",
    "\n",
    "        conv2 = self.dconv_down2(x)          \n",
    "        x = self.maxpool(conv2)     \n",
    "\n",
    "        conv3 = self.dconv_down3(x)          \n",
    "        x = self.maxpool(conv3)     \n",
    "\n",
    "        x = self.dconv_down4(x)    \n",
    "        x = self.upsample(x)        \n",
    "        # print('La taille de x est ', x.shape, 'et la taille de conv3 est ', conv3.shape)\n",
    "        x = tch.cat([x, conv3], dim=1) \n",
    "\n",
    "        x = self.dconv_up3(x)\n",
    "        x = self.upsample(x)\n",
    "        # print('La taille de x est ', x.shape, 'et la taille de conv2 est ', conv2.shape)\n",
    "        x = tch.cat([x, conv2], dim=1)\n",
    "\n",
    "        x = self.dconv_up2(x)\n",
    "        x = self.upsample(x)\n",
    "        #  print('La taille de x est ', x.shape, 'et la taille de conv1 est ', conv3.shape)\n",
    "        x = tch.cat([x, conv1], dim=1)\n",
    "\n",
    "        x = self.dconv_up1(x)\n",
    "        out = self.conv_last(x)\n",
    "        return out\n",
    "    \n",
    "\n",
    "unet_model = UNet(in_channels = 3, out_channels = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had a problem in the concatening steps, the size doesn't match. After investigating the problem, we found that at a step the height size became odd and so the division by 2 let us loss a range of pixel on the height and after that it creates an offset with the mutltiplication so instead of cropping twice inside the model at each stacjking steep with an issue, we found it better to crop 2 range of pixels from each image on the height side in the dataset's generation, hence the height will be more divisible by 2 (more 2 in the prime factors decomposition).\n",
    "\n",
    "Also in that case, we could change our custom Unet model back to the usual one if needed without changing more the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Testing the architecture\n",
    "# unet_model.eval()\n",
    "\n",
    "# image, mask = test_data[0]\n",
    "\n",
    "# with tch.no_grad():\n",
    "#     predictions = unet_model(image.unsqueeze(0))\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Image(z=np.concatenate((image.squeeze(0).permute(1, 2, 0)*255, (predictions.squeeze(0)).permute(1, 2, 0)*255),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définir la fonction de perte (criterion) et l'optimiseur\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = tch.optim.Adam(unet_model.parameters(), lr=0.001)\n",
    "\n",
    "unet_model.train()\n",
    "\n",
    "n_epoch = 10\n",
    "\n",
    "\n",
    "for epoch in range(n_epoch) :\n",
    "    running_loss = 0.0\n",
    "    for image, mask in data_loader :\n",
    "        # Remettre à zéro les gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        pred = unet_model(image)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(pred, mask)\n",
    "\n",
    "        # Backpropagation and update of the weights\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate the whole loss of the epoch\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Afficher la perte moyenne de l'époque\n",
    "    print(f\"Epoch [{epoch+1}/{n_epoch}], Loss: {running_loss/len(data_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH = Path(\"%pwd\").resolve().parent / 'unet_model.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model saving\n",
    "# tch.save(unet_model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model Loading\n",
    "# unet_model = UNet(in_channels = 3, out_channels = 3)\n",
    "# unet_model.load_state_dict(tch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_model.eval()\n",
    "\n",
    "# image, mask = test_data[1]\n",
    "\n",
    "# with tch.no_grad():\n",
    "#     predictions = unet_model(image.unsqueeze(0))\n",
    "# fig = go.Figure()\n",
    "# fig.add_trace(go.Image(z=np.concatenate((image.squeeze(0).permute(1, 2, 0)*255, (predictions.squeeze(0)).permute(1, 2, 0)*255),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Évaluation sur l'ensemble de test\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "with tch.no_grad():\n",
    "    for i in len(test_data) :\n",
    "        image, mask = test_data[0,i], test_data[1,i]\n",
    "        outputs = unet_model(image)\n",
    "        _, predicted = tch.max(outputs, 1)\n",
    "        total_correct += (predicted == mask).sum().item()\n",
    "        total_samples += mask.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculer la précision\n",
    "accuracy = total_correct / total_samples\n",
    "print(f\"Accuracy on test set: {accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "cnn-py3.10",
   "name": "workbench-notebooks.m118",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m118"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
